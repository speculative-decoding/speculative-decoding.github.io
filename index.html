<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLING 2025 Tutorial: Speculative Decoding for Efficient LLM Inference</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">COLING 2025 Tutorial:</span><br />
              Speculative Decoding for Efficient LLM Inference
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
            <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_heming.jpg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_yongqi.jpg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_cunxiao.jpg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_qian.png"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_maggie.jpg"></td>
            </tr>
              <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="20%" style="text-align: center"><a href="https://hemingkx.github.io/" style="border-radius: 50%">Heming Xia</a><sup>1</sup>,</td>
                <td width="20%" style="text-align: center"><a href="https://liyongqi67.github.io/" style="border-radius: 50%">Yongqi Li</a><sup>1</sup>,</td>
                <td width="20%" style="text-align: center"><a href="https://nonvolatilememory.github.io/" style="border-radius: 50%">Cunxiao Du</a><sup>2</sup>,</td>
                <td width="20%" style="text-align: center"><a href="https://siviltaram.github.io/" style="border-radius: 50%">Qian Liu</a><sup>3</sup></td>
                <td width="20%" style="text-align: center"><a href="https://www4.comp.polyu.edu.hk/~cswjli/" style="border-radius: 50%">Wenjie Li</a><sup>1</sup></td>
              </tr>
            </table>
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong Polytechnic University,</span>
            <span class="author-block"><sup>2</sup>SEA AI Lab,</span>
            <span class="author-block"><sup>3</sup>TikTok</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Sunday, January 19, 09:00 - 12:30 (GST), Tutorial 1</b>
          </div>
	      <div class="is-size-5 publication-authors">
            <b>Abu Dhabi National Exhibition Centre, Capital Suite 7</b>
          </div>

          <!--<div class="is-size-5 publication-authors">
            Visit <a target="_blank" href="https://us06web.zoom.us/rec/play/6fqU9YDLoFtWqpk8w8I7oFrszHKW6JkbPVGgHsdPBxa69ecgCxbmfP33asLU3DJ74q5BXqDGR2ycOTFk.93teqylfi_uiViNK?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fus06web.zoom.us%2Frec%2Fshare%2FNrYheXPtE5zOlbogmdBg653RIu7RBO1uAsYH2CZt_hacD1jOHksRahGlERHc_Ybs.KGX1cRVtJBQtJf0o">this link</a>
            for the Zoom recording of the tutorial
          </div>-->
          <div class="is-size-5 publication-authors">
            QnA: <a href="https://tinyurl.com/spec-tutorial" target="_blank"><b>tinyurl.com/spec-tutorial</b></a>
          </div>
          <div class="is-size-6 publication-authors">
            Slides and video recordings will be released after the tutorial.
          </div>
          <br />
          <!--<div class="is-size-5 publication-authors">
            QnA: <a href="https://tinyurl.com/retrieval-lm-tutorial" target="_blank"><b>tinyurl.com/retrieval-lm-tutorial</b></a>
          </div>-->
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            This tutorial presents a comprehensive introduction to Speculative Decoding (SD),
            an advanced technique for LLM inference acceleration that has garnered significant research interest in recent years.
            SD is introduced as an innovative decoding paradigm to mitigate the high inference latency stemming from autoregressive decoding in LLMs.
            At each decoding step, SD efficiently drafts several future tokens and then verifies them in parallel.
            This approach, unlike traditional autoregressive decoding, facilitates the simultaneous decoding of multiple tokens per step,
            thereby achieving promising 2x-4x speedups in LLM inference while maintaining original distributions.
          </p>
          <p>
            This tutorial delves into the latest techniques in SD, including draft model architectures and verification strategies.
            Additionally, it explores the acceleration potential and future research directions in this promising field.
            We aim for this tutorial to elucidate the current research landscape and offer insights for researchers interested in Speculative Decoding,
            ultimately contributing to more efficient LLM inference.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        <p>
          Our tutorial will be held on January 19 (all the times are based on GST = Abu Dhabi local time).
          <!-- <em>Slides may be subject to updates.</em>-->
        </p>

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">09:00—09:40</td>
              <td class="tg-0lax">Part I: Introduction & Definition <!-- <a href="./slides/1-intro.pdf" target='_blank'>[Slides]</a>--></td>
              <td class="tg-0lax">Heming</td>
            </tr>
            <tr>
              <td class="tg-0lax">09:40—10:25</td>
              <td class="tg-0lax">Part II: History and A Taxonomy of Methods </td>
              <td class="tg-0lax">Qian</td>
            </tr>
            <tr>
              <td class="tg-0lax">10:25—10:30</td>
              <td class="tg-0lax">Q & A Session I</td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">10:30—11:00</td>
              <td class="tg-0lax">Coffee break</td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">11:00—11:40</td>
              <td class="tg-0lax">Part III: Cutting-edge Algorithms </td>
              <td class="tg-0lax">Heming</td>
            </tr>
            <tr>
              <td class="tg-0lax">11:40—12:10</td>
              <td class="tg-0lax">Part IV: Downstream Adaptations </td>
              <td class="tg-0lax">Yongqi</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:10—12:30</td>
              <td class="tg-0lax">Part V: Final Remarks and Outlook + Q & A Session II </td>
              <td class="tg-0lax">Yongqi</td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <p><b>Bold papers</b> are discussed in detail during our tutorial.</p>

        <p>For more details, we recommend that readers refer to our <a href="https://aclanthology.org/2024.findings-acl.456/"><b>Survey</b></a> and <a href="https://github.com/hemingkx/SpeculativeDecodingPapers"><b>Reading List</b></a> on Speculative Decoding.</p>

        <br />
        
        <h3 class="title is-5">Part II: History</h3>

        <ul>
          <li><a href="https://proceedings.neurips.cc/paper/2018/hash/c4127b9194fe8562c64dc0f5bf2c93bc-Abstract.html"><b>Blockwise Parallel Decoding for Deep Autoregressive Models</b></a> (Stern et al., 2018)</li>
          <li><a href="https://aclanthology.org/2023.findings-emnlp.257"><b>Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation</b></a> (Xia et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2211.17192"><b>Fast Inference from Transformers via Speculative Decoding</b></a> (Leviathan et al., 2022)</li>
          <li><a href="http://arxiv.org/abs/2302.01318"><b>Accelerating Large Language Model Decoding with Speculative Sampling</b></a> (Chen et al., 2023)</li>
          <li><a href="https://aclanthology.org/2021.acl-long.462/">Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding</a> (Sun et al., 2021)</li>
          <li><a href="http://arxiv.org/abs/2304.04487">Inference with Reference: Lossless Acceleration of Large Language Models</a> (Yang et al., 2023)</li>
          <li><a href="https://huggingface.co/blog/assisted-generation">Assisted Generation: a new direction toward low-latency text generation</a> (Huggingface, 2023)</li>
          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/7b97adeafa1c51cf65263459ca9d0d7c-Paper-Conference.pdf">Speculative Decoding with Big Little Decoder</a> (Kim et al., 2023)</li>
        </ul>
        
        <br />

        <h3 class="title is-5">Part II: A Taxonomy of Methods</h3>

        <ul>
          <li><a href="https://aclanthology.org/2024.findings-acl.456/"><b>Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding</b></a> (Xia et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2309.08168"><b>Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding</b></a> (Zhang et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2311.08252"><b>REST: Retrieval-Based Speculative Decoding</b></a> (He et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2401.10774"><b>Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</b></a> (Cai et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2401.15077"><b>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</b></a> (Li et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2402.02057"><b>Break the Sequential Dependency of LLM Inference Using Lookahead Decoding</b></a> (Fu et al., 2024)</li>
          <li><a href="https://arxiv.org/abs/2305.10427">Accelerating Transformer Inference for Translation via Parallel Decoding</a> (Santilli et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2404.16710">Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding</a> (Elhoushi et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2310.08461">DistillSpec: Improving Speculative Decoding via Knowledge Distillation</a> (Zhou et al., 2023)</li>
        </ul>

        <br />

        <h3 class="title is-5">Part III: Cutting-edge Algorithms</h3>

        <ul>
          <li><a href="http://arxiv.org/abs/2402.02082"><b>GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding</b></a> (Du et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2406.16858"><b>EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</b></a> (Li et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2410.06916"><b>SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration</b></a> (Xia et al., 2024)</li>
          <li><a href="https://arxiv.org/abs/2411.10666"><b>SAM Decoding: Speculative Decoding via Suffix Automaton</b></a> (Hu et al., 2024)</li>
          <li><a href="https://openreview.net/forum?id=mtSSFiqW6y">Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment</a> (ICLR 2025 Submission)</li>
          <li><a href="http://arxiv.org/abs/2406.17276">OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure</a> (Wang et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2402.13720">Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding</a> (Zhao et al., 2024)</li>
        </ul>

        <br />

        <h3 class="title is-5">Part IV: Downstream Adaptations</h3>
        
        <ul>
          <li><a href="http://arxiv.org/abs/2404.11912"><b>TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding</b></a> (Sun et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2401.14021"><b>Accelerating Retrieval-Augmented Language Model Serving with Speculation</b></a> (Zhang et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2406.16758"><b>Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters</b></a> (Yi et al., 2023)</li>
          <li><a href="http://arxiv.org/abs/2410.03355">LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding</a> (Jang et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2410.05165">Efficient Inference for Large Language Model-based Generative Recommendation</a> (Lin et al., 2024)</li>
          <li><a href="http://arxiv.org/abs/2407.08223">Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting</a> (Wang et al., 2024)</li>
        </ul>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ speculative-decoding-tutorial,
  author    = { Xia, Heming and Du, Cunxiao and Li, Yongqi and Liu, Qian and Li, Wenjie },
  title     = { COLING 2025 Tutorial: Speculative Decoding for Efficient LLM Inference },
  journal   = { COLING 2025 },
  year      = { 2025 },
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/speculative-decoding" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
